import torch
from torch import nn
from torchvision import datasets
from torchvision.transforms import ToTensor
from torch.utils.data import DataLoader
from torchmetrics import Accuracy

import matplotlib.pyplot as plt
from pathlib import Path
from typing import Tuple
from timeit import default_timer as timer
from tqdm.auto import tqdm

torch.manual_seed(42)

BATCH_SIZE = 32
DEVICE = 'cpu' # <'mps', 'cpu'>
#/Applications/miniconda3/envs/vision/lib/python3.10/site-packages/torch/functional.py:799: 
# UserWarning: MPS: _unique2 op is supported natively starting from macOS 13.0. Falling back on CPU. 
# This may have performace implications. (Triggered internally at 
# /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1673597342637/work/aten/src/ATen/native/mps/operations/Unique.mm:356.)
#  output, inverse_indices, counts = torch._unique2(

EPOCHS=3
IMG_SIZE = 28

data_path = Path('data')
data_path.mkdir(exist_ok=True)

def print_train_time(start: float, end: float, device: torch.device = None):
    """Prints difference between start and end time.

    Args:
        start (float): Start time of computation (preferred in timeit format). 
        end (float): End time of computation.
        device ([type], optional): Device that compute is running on. Defaults to None.

    Returns:
        float: time between start and end in seconds (higher is longer).
    """
    total_time = end - start
    print(f"Train time on {device}: {total_time:.3f} seconds")
    return total_time

train_dataset = datasets.FashionMNIST(root='data',
                                      train=True,
                                      transform=ToTensor(),
                                      target_transform=None,
                                      download=True)

test_dataset = datasets.FashionMNIST(root='data',
                                     train=False,
                                     transform=ToTensor(),
                                     download=True)

class_names = train_dataset.classes

# img, label = test_dataset[0]
# print(img.size(), img.squeeze().size())
# plt.imshow(img.squeeze(), cmap='gray')

train_dataloader = DataLoader(train_dataset, 
                              batch_size=BATCH_SIZE,
                              shuffle=True
                              )
test_dataloader = DataLoader(test_dataset,
                             batch_size=BATCH_SIZE,
                             shuffle=True
                             )

class ConvModel(nn.Module):
    def __init__(self, color_channels: int, input_size: int, output_shape: int,  hidden_units: int = 10) -> None:
        super().__init__()
        self.conv_block1 = nn.Sequential(
            nn.Conv2d(in_channels=color_channels, out_channels=hidden_units,
                      kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units,
                      kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.conv_block2 = nn.Sequential(
            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units,
                      kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units,
                      kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
            
        )
        
        self.classifier_block = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_features=int(hidden_units * ((input_size / 4) ** 2)), out_features=output_shape), # / 4 because we use 2 times nn.MaxPool2d(kernel_size=2)
        )
        
    def forward(self, X: torch.Tensor) -> torch.Tensor:
        X = self.conv_block1(X)
        X = self.conv_block2(X)
        y = self.classifier_block(X)
        return y


# # test network structure
# base_model = ConvModel(color_channels=1, input_size=64, output_shape=len(class_names), hidden_units=10).to(DEVICE)
# print(base_model)
# X = torch.randn(size=(32, 1, 64, 64))
# y_pred = base_model(X)

conv_model =  ConvModel(color_channels=1, input_size=IMG_SIZE, output_shape=len(class_names), hidden_units=10).to(DEVICE)

accuracy = Accuracy(task='multiclass', num_classes=len(class_names)).to(DEVICE)
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(params=conv_model.parameters(), 
                            lr=1e-1)

start = timer()
for epoch in range(EPOCHS):
    print(f'EPOCH: {epoch}')
    
    # TRAINING
    conv_model.train()
    train_loss, train_acc = 0, 0
    train_batches = len(train_dataloader)
    for X_train, y_train in train_dataloader:
        X_train, y_train = X_train.to(DEVICE), y_train.to(DEVICE)
        y_train_logits = conv_model(X_train)
        y_train_pred = torch.argmax(y_train_logits, dim=1)
        loss = loss_fn(y_train_logits, y_train)
        acc = accuracy(y_train_pred, y_train.type(torch.LongTensor))
        train_loss += loss
        train_acc += acc
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    train_loss = train_loss / train_batches
    train_acc = train_acc / train_batches
    print(f'\tTRAIN loss: {train_loss:.5f} accuracy: {train_acc:.2f}')
    
    # EVALUATION
    conv_model.eval()
    test_loss, test_acc = 0, 0
    test_batches = len(test_dataloader)
    for X_test, y_test in test_dataloader:
        X_test, y_test = X_test.to(DEVICE), y_test.to(DEVICE)
        with torch.inference_mode():
            y_test_logits = conv_model(X_test)
            y_test_pred = torch.argmax(y_test_logits, dim=1)
            test_loss += loss_fn(y_test_logits, y_test)
            test_acc += accuracy(y_test_pred, y_test.type(torch.LongTensor))
    test_loss = test_loss / test_batches
    test_acc = test_acc / test_batches 
    print(f'\tTEST loss: {test_loss:.5f} accuracy: {test_acc:.2f}')

print_train_time(start=start, end=timer(), device=DEVICE)

# SAVE MODEL
models_path = Path('models')
models_path.mkdir(parents=True, exist_ok=True)

MODEL_NAME = 'FashionMNIST_ConvNet_TinyVGG.pth'
MODEL_PATH = models_path / MODEL_NAME

torch.save(conv_model.state_dict(), f=MODEL_PATH)